{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - \"The Office\" dataset\n",
    "This notebook aims to provide parameterizable functions to preprocess the \"The Office\" dataset for further NLP analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, MWETokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "PATH = \"../data/\"\n",
    "FILE = \"the-office-lines_scripts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH+FILE, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate line_text for each scene\n",
    "def concatenate_scenes(df):\n",
    "    df = df.groupby([\"season\", \"episode\", \"scene\"])[\"line_text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_directorals(df):\n",
    "    # extract text from line_text in square brackets, put it in new column called \"directionals\", multiple square brackets will be extracted as a list\n",
    "    df[\"directionals\"] = df[\"line_text\"].str.extractall(r\"\\[(.*?)\\]\").unstack().apply(lambda x: \", \".join(x.dropna()), axis=1)\n",
    "    # delete the extracted text from line_text\n",
    "    df[\"line_text\"] = df[\"line_text\"].str.replace(r\"\\[(.*?)\\]\", \"\", regex=True).str.strip()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bare string preprocessing\n",
    "def remove_punctuation(df):\n",
    "    return df[\"line_text\"].str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "def lower(df):\n",
    "    return df[\"line_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return df[\"line_text\"].apply(lambda x: \" \".join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "\n",
    "def expanding_contractions(df):\n",
    "    return df[\"line_text\"].apply(lambda x: contractions.fix(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, tokenizer=\"TreeBankWord\", tokenize_specialwords=True):\n",
    "    if tokenizer==\"TreeBankWord\":\n",
    "        t = nltk.tokenize.TreebankWordTokenizer()\n",
    "    elif tokenizer==\"WordPunct\":\n",
    "        t = nltk.tokenize.WordPunctTokenizer()\n",
    "    elif tokenizer==\"Whitespace\":\n",
    "        t = nltk.tokenize.WhitespaceTokenizer()\n",
    "    else:\n",
    "        raise ValueError(f\"Tokenizer {tokenizer} does not exist.\")\n",
    "\n",
    "    tmp = df[\"line_text\"].apply(lambda x: t.tokenize(x))\n",
    "\n",
    "    if tokenize_specialwords:\n",
    "        names = pd.read_csv(PATH+\"character_names.csv\", sep=\";\", encoding='cp1252').Character.values\n",
    "        names = names.tolist()\n",
    "        names.extend([name.lower() for name in names])\n",
    "        with open(PATH+\"compound_words_the-office_by_chatgpt.txt\", \"r\") as f:\n",
    "            compound_words = f.read().split(\",\")\n",
    "        compound_words = [word.strip() for word in compound_words]\n",
    "        compound_words.extend([w.lower() for w in compound_words])\n",
    "        special_words = names + compound_words\n",
    "        special_tokenizer = MWETokenizer([w.split(\" \") for w in special_words])\n",
    "        return tmp.apply(lambda x: special_tokenizer.tokenize(x))\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def lemmatize(df):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # is working, but not very good results because of the simple speech of the characters\n",
    "    return df[\"line_text\"].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
    "\n",
    "def stem(df):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return df[\"line_text\"].apply(lambda x: \" \".join([porter_stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "\n",
    "# tagging\n",
    "def pos_tag(df):\n",
    "    return df[\"line_text\"].apply(lambda x: nltk.pos_tag(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        df, \n",
    "        concat_scenes=False, \n",
    "        extract_direc=False, \n",
    "        remove_punct=False, \n",
    "        rmv_stopwords=False, \n",
    "        lwr=False, \n",
    "        exp_contractions=False, \n",
    "        conversion:str=None,\n",
    "        tokenizer=(None, False) # parameter for tokenize function (tokenizer(string), tokenize_specialwords(bool)), only used if conversion is \"tokenize\"\n",
    "        )->pd.DataFrame:\n",
    "    if (concat_scenes):\n",
    "        df = concatenate_scenes(df)\n",
    "    if (extract_direc):\n",
    "        df = extract_directorals(df)\n",
    "\n",
    "    if (remove_punct):\n",
    "        df['line_text'] = remove_punctuation(df)\n",
    "    if (lwr):\n",
    "        df['line_text'] = lower(df)\n",
    "    if (rmv_stopwords):\n",
    "        df['line_text'] = remove_stopwords(df)\n",
    "    if (exp_contractions):\n",
    "        df['line_text'] = expanding_contractions(df)   \n",
    "\n",
    "    if (conversion == \"tokenize\"):\n",
    "        df['line_text']  = tokenize(df, tokenizer[0], tokenizer[1])\n",
    "    elif (conversion == \"lemmatize\"):\n",
    "        df['line_text'] = lemmatize(df)\n",
    "    elif (conversion == \"stem\"):\n",
    "        df['line_text'] = stem(df)\n",
    "    elif (conversion == \"pos_tag\"):\n",
    "        df['line_text'] = pos_tag(df)\n",
    "\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "param_dict = {\n",
    "    \"concat_scenes\": True,\n",
    "    \"extract_direc\": True, \n",
    "    \"remove_punct\": True, \n",
    "    \"rmv_stopwords\": True,\n",
    "    \"lwr\": True, \n",
    "    \"exp_contractions\": True,\n",
    "    \"conversion\": \"tokenize\",\n",
    "    \"tokenizer\": (\"TreeBankWord\", True)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>line_text</th>\n",
       "      <th>directionals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[right, jim, quarterlies, look, good, things, library, oh, told, could, close, come, master, guidance, saying, grasshopper, actually, called, yeah, right, well, let, show, done]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[phone, yes, id, like, speak, office, manager, please, yes, hello, michael_scott, regional_manager, dunder_mifflin, paper, products, wanted, talk, manageramanger, quick, cut, scene, right, done, deal, thank, much, sir, gentleman, scholar, oh, sorry, ok, sorry, mistake, hangs, woman, talking, low, voice, probably, smoker, clears, throat, way, done]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[uh, dunder_mifflin, 12, years, last, four, regional_manager, want, come, see, entire, floor, kingdom, far, eye, see, receptionist, pam, pam, pampam, pam_beesly, pam, us, forever, right, pam, well, know, think, cute, seen, couple, years, ago, growls, messages, uh, yeah, fax, oh, pam, corporate, many, times, told, special, filing, cabinet, things, corporate, told, called, wastepaper, basket, look, look, face]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[people, say, best, boss, go, god, never, worked, place, like, hilarious, get, best, us, shows, camera, worlds, best, boss, mug, think, pretty, much, sums, found, spencer, gifts]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[singing, shall, play, pa, rum, pump, um, pum, imitates, heavy, drumming, gifts, pa, rum, pump, um, pum, imitates, heavy, drumming]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9156</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>[seems, arbitrary, applied, job, company, hiring, took, desk, back, empty, chuckles, matter, get, end, human, beings, miraculous, gift, make, place, home, standing, two, cops, let]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9157</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>113</td>\n",
       "      <td>[feel, lucky, got, chance, share, crummy, story, anyone, thinks, one, take, dump, paper, shredder, alone, sister, let, get, beer, sometime]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9158</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>114</td>\n",
       "      <td>[happy, filmed, remember, everyone, worked, paper, company, years, never, wrote, anything]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>115</td>\n",
       "      <td>[sold, paper, company, 12, years, job, speak, clients, phone, quantities, types, copier, paper, even, love, every, minute, everything, owe, job, stupid, wonderful, boring, amazing, job]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>116</td>\n",
       "      <td>[thought, weird, picked, us, make, documentary, think, ordinary, paper, company, like, dunder_mifflin, great, subject, documentary, lot, beauty, ordinary, things, kind, point]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9161 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      season  episode  scene  \\\n",
       "0          1        1      1   \n",
       "1          1        1      2   \n",
       "2          1        1      3   \n",
       "3          1        1      4   \n",
       "4          1        1      5   \n",
       "...      ...      ...    ...   \n",
       "9156       9       23    112   \n",
       "9157       9       23    113   \n",
       "9158       9       23    114   \n",
       "9159       9       23    115   \n",
       "9160       9       23    116   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                        line_text  \\\n",
       "0                                                                                                                                                                                                                                               [right, jim, quarterlies, look, good, things, library, oh, told, could, close, come, master, guidance, saying, grasshopper, actually, called, yeah, right, well, let, show, done]   \n",
       "1                                                                   [phone, yes, id, like, speak, office, manager, please, yes, hello, michael_scott, regional_manager, dunder_mifflin, paper, products, wanted, talk, manageramanger, quick, cut, scene, right, done, deal, thank, much, sir, gentleman, scholar, oh, sorry, ok, sorry, mistake, hangs, woman, talking, low, voice, probably, smoker, clears, throat, way, done]   \n",
       "2     [uh, dunder_mifflin, 12, years, last, four, regional_manager, want, come, see, entire, floor, kingdom, far, eye, see, receptionist, pam, pam, pampam, pam_beesly, pam, us, forever, right, pam, well, know, think, cute, seen, couple, years, ago, growls, messages, uh, yeah, fax, oh, pam, corporate, many, times, told, special, filing, cabinet, things, corporate, told, called, wastepaper, basket, look, look, face]   \n",
       "3                                                                                                                                                                                                                                              [people, say, best, boss, go, god, never, worked, place, like, hilarious, get, best, us, shows, camera, worlds, best, boss, mug, think, pretty, much, sums, found, spencer, gifts]   \n",
       "4                                                                                                                                                                                                                                                                                             [singing, shall, play, pa, rum, pump, um, pum, imitates, heavy, drumming, gifts, pa, rum, pump, um, pum, imitates, heavy, drumming]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                           ...   \n",
       "9156                                                                                                                                                                                                                                         [seems, arbitrary, applied, job, company, hiring, took, desk, back, empty, chuckles, matter, get, end, human, beings, miraculous, gift, make, place, home, standing, two, cops, let]   \n",
       "9157                                                                                                                                                                                                                                                                                  [feel, lucky, got, chance, share, crummy, story, anyone, thinks, one, take, dump, paper, shredder, alone, sister, let, get, beer, sometime]   \n",
       "9158                                                                                                                                                                                                                                                                                                                                   [happy, filmed, remember, everyone, worked, paper, company, years, never, wrote, anything]   \n",
       "9159                                                                                                                                                                                                                                    [sold, paper, company, 12, years, job, speak, clients, phone, quantities, types, copier, paper, even, love, every, minute, everything, owe, job, stupid, wonderful, boring, amazing, job]   \n",
       "9160                                                                                                                                                                                                                                              [thought, weird, picked, us, make, documentary, think, ordinary, paper, company, like, dunder_mifflin, great, subject, documentary, lot, beauty, ordinary, things, kind, point]   \n",
       "\n",
       "      directionals  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  \n",
       "...            ...  \n",
       "9156           NaN  \n",
       "9157           NaN  \n",
       "9158           NaN  \n",
       "9159           NaN  \n",
       "9160           NaN  \n",
       "\n",
       "[9161 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = preprocess(df, **param_dict)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "def extract_features(df, vectorizer):\n",
    "    if vectorizer == \"binary\":\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "    elif vectorizer == \"count\":\n",
    "        vectorizer = CountVectorizer() \n",
    "    elif vectorizer == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif vectorizer == \"hashing\":\n",
    "        vectorizer = HashingVectorizer()\n",
    "\n",
    "    result = vectorizer.fit_transform(df[\"line_text\"])\n",
    "    return result\n",
    "\n",
    "def feature_selection (feature_df, selection_method):\n",
    "    # TODO: add feature selection e.g. DF (document frequency)\n",
    "    print(\"nothin here yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59911, 22850)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature extraction\n",
    "param_dict = {\n",
    "    \"concat_scenes\": False,\n",
    "    \"extract_direc\": False, \n",
    "    \"remove_punct\": True, \n",
    "    \"rmv_stopwords\": False,\n",
    "    \"lwr\": True, \n",
    "    \"exp_contractions\": True,\n",
    "    \"conversion\": \"lemmmatize\"\n",
    "}\n",
    "test = preprocess(df, **param_dict)\n",
    "feature_df = extract_features(test, \"count\")\n",
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "to_csv not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# save the preprocessed data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df\u001b[39m.\u001b[39mto_csv(PATH\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpreprocessed_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mFILE, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m feature_df\u001b[39m.\u001b[39;49mto_csv(PATH\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeature_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mFILE, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\sparse\\_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[0;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: to_csv not found"
     ]
    }
   ],
   "source": [
    "# save the preprocessed data\n",
    "df.to_csv(PATH+\"preprocessed_\"+FILE, sep=\",\", index=True)\n",
    "feature_df.to_csv(PATH+\"feature_\"+FILE, sep=\",\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "885d594eef47ba9d029fc86f8c38af7ec9523f1d4e741834c77e8fd404d0fae1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
